{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b939aded",
      "metadata": {
        "id": "b939aded"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# download data from from Google Drive\n",
        "!gdown 1uml6lt5diPDjBnJYVECBrafVJEISRU2-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file in to a list of tokens\n",
        "from collections import defaultdict\n",
        "noun_tokens=defaultdict(int)\n",
        "verb_tokens=defaultdict(int)\n",
        "tokens=[]\n",
        "filename=\"Manchester-morph+changes_utterancesremoved.cha\"\n",
        "with open(filename) as file:\n",
        "        for line in file:\n",
        "            if re.match(\"^\\*MOT\",line):\n",
        "                line=re.sub('\\*[A-Z]+\\:','', line)\n",
        "                line=re.sub(' \\.',' ', line)\n",
        "                line=re.sub(' \\?',' ', line)\n",
        "                line=re.sub(' \\,',' ', line)\n",
        "                line=re.sub(' \\!',' ', line)\n",
        "                line = \"{ \" + line + \" } \"\n",
        "                tokens.extend(line.split())\n",
        "\n",
        "            if re.match(\"^\\%mor\",line):\n",
        "                line=re.sub('\\%mor\\:','', line)\n",
        "                mortokens = line.split()\n",
        "                for tok in mortokens:\n",
        "                     if re.match(\"^noun\\|\",tok):\n",
        "                        tok=re.sub(\"^noun\\|(.+)\",'\\\\1', tok)\n",
        "                        noun_tokens[str.lower(tok)] += 1\n",
        "                     elif re.match(\"^verb\\|\",tok):\n",
        "                        tok=re.sub(\"^verb\\|(.+)\",'\\\\1', tok)\n",
        "                        verb_tokens[str.lower(tok)] += 1\n"
      ],
      "metadata": {
        "id": "RpQrzHTRg3-9"
      },
      "id": "RpQrzHTRg3-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_noun_counts=sorted(noun_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "sorted_verb_counts=sorted(verb_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "sorted_noun_tokens=list(zip(*sorted_noun_counts))[0]\n",
        "sorted_verb_tokens=list(zip(*sorted_verb_counts))[0]\n",
        "excluded_nouns=[\"mummy\",\"daddy\",\"john\",\"carl\",\"dominic\"] # proper names\n",
        "excluded_verbs=[\"\"]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if x not in excluded_nouns]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if noun_tokens[x] > verb_tokens[x]]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if x not in excluded_verbs]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if verb_tokens[x] > noun_tokens[x]]\n",
        "seed_set_size = 10\n",
        "noun_seeds=sorted_noun_tokens[0:seed_set_size]\n",
        "verb_seeds=sorted_verb_tokens[0:seed_set_size]\n",
        "type_list=list(noun_seeds)\n",
        "type_list.extend(verb_seeds)\n",
        "tokens.insert(0,\"{\")\n",
        "tokens.insert(len(tokens),\"}\")\n",
        "token_count=len(tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhnJGJPLifUL"
      },
      "id": "WhnJGJPLifUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bfcf79",
      "metadata": {
        "id": "33bfcf79"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "type_count = len(type_list)\n",
        "targets = []\n",
        "positions = []\n",
        "contextwordcounts = []\n",
        "d = defaultdict(lambda: [0] * (seed_set_size*2))\n",
        "window_size = 2\n",
        "#M = np.zeros((type_count, 4, type_count))\n",
        "toks_to_ignore = [\"{\",\"}\"]\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
        "            begin = max(i - window_size, 0)\n",
        "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
        "            end  = min(i + window_size, token_count)\n",
        "            # Extract the text from beginning of window to the end\n",
        "            context = tokens[begin: end + 1]\n",
        "            # Remove the target word from its own window\n",
        "            context.remove(tokens[i])\n",
        "            context=[\"noun\" if noun_seeds.count(w) > 0 else w for w in context]\n",
        "            context=[\"verb\" if verb_seeds.count(w) > 0 else w for w in context]\n",
        "            if type_list.count(word) > 0:\n",
        "                target_id = type_list.index(word)\n",
        "            #if toks_to_ignore.count(word) == 0:\n",
        "                p1 = context[1] + \"_X_\" + context[2]\n",
        "                p1 = re.sub(\".+(\\{.+)\",\"\\\\1\",p1)\n",
        "                p1 = re.sub(\"(.+\\}).+\",\"\\\\1\",p1)\n",
        "\n",
        "                p1a = context[1] + \"_X_\"\n",
        "                p1a = re.sub(\".+(\\{.+)\",\"\\\\1\",p1a)\n",
        "                p1a = re.sub(\"(.+\\}).+\",\"\\\\1\",p1a)\n",
        "\n",
        "                p2 = \"X_\" + context[2] + \"_\" + context[3]\n",
        "                p2 = re.sub(\".+(\\{.+)\",\"\\\\1\",p2)\n",
        "                p2 = re.sub(\"(.+\\}).+\",\"\\\\1\",p2)\n",
        "\n",
        "                p2a = \"X_\" + context[2]\n",
        "                p2a = re.sub(\".+(\\{.+)\",\"\\\\1\",p2a)\n",
        "                p2a = re.sub(\"(.+\\}).+\",\"\\\\1\",p2a)\n",
        "\n",
        "                p3 = context[0] + \"_\" + context[1] + \"_X\"\n",
        "                p3 = re.sub(\".+(\\{.+)\",\"\\\\1\",p3)\n",
        "                p3 = re.sub(\"(.+\\}).+\",\"\\\\1\",p3)\n",
        "\n",
        "                # Find the ID and hence the list index for the current token\n",
        "                #token_id = type_list.index(token)\n",
        "                # Add 1 to the current context dimension for the current target word\n",
        "                d[p1][target_id] += 1\n",
        "                d[p2][target_id] += 1\n",
        "                d[p3][target_id] += 1\n",
        "                d[p1a][target_id] += 1\n",
        "                d[p2a][target_id] += 1\n",
        "\n",
        "                #M[current_row][position][token_id] += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a hierachical data frame\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(d)\n",
        "df.index = type_list"
      ],
      "metadata": {
        "id": "aXy2ATpqbik6"
      },
      "id": "aXy2ATpqbik6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "vizu46vQdW0n"
      },
      "id": "vizu46vQdW0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "window_size = 2\n",
        "toks_to_ignore = [\"{\",\"}\"]\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
        "            begin = max(i - window_size, 0)\n",
        "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
        "            end  = min(i + window_size, token_count)\n",
        "            # Extract the text from beginning of window to the end\n",
        "            context = tokens[begin: end + 1]\n",
        "            # Remove the target word from its own window\n",
        "            context.remove(tokens[i])\n",
        "            context=[\"noun\" if noun_seeds.count(w) > 0 else w for w in context]\n",
        "            context=[\"verb\" if verb_seeds.count(w) > 0 else w for w in context]\n",
        "\n",
        "            if toks_to_ignore.count(word) == 0:\n",
        "                p1 = context[1] + \"_X_\" + context[2]\n",
        "                p1 = re.sub(\".+(\\{.+)\",\"\\\\1\",p1)\n",
        "                p1 = re.sub(\"(.+\\}).+\",\"\\\\1\",p1)\n",
        "                p1a = context[1] + \"_X\"\n",
        "                p1a = re.sub(\".+(\\{.+)\",\"\\\\1\",p1a)\n",
        "                p1a = re.sub(\"(.+\\}).+\",\"\\\\1\",p1a)\n",
        "                if list(df.columns).count(p1) > 0:\n",
        "                    #print(p1)\n",
        "                    #print(df.get(p1))\n",
        "                    #print(df.get(p1).idxmax())\n",
        "                    if sorted_noun_tokens.count(df.get(p1).idxmax()) > 0:\n",
        "                        category=\"noun\"\n",
        "                    if sorted_verb_tokens.count(df.get(p1).idxmax()) > 0:\n",
        "                        category=\"verb\"\n",
        "                elif list(df.columns).count(p1a) > 0:\n",
        "                    if sorted_noun_tokens.count(df.get(p1a).idxmax()) > 0:\n",
        "                        category=\"noun\"\n",
        "                    if sorted_verb_tokens.count(df.get(p1a).idxmax()) > 0:\n",
        "                        category=\"verb\"\n",
        "                else:\n",
        "                    category = \"other\"\n",
        "                print(word + \" \" + category)\n"
      ],
      "metadata": {
        "id": "UA9apDhCglMJ"
      },
      "id": "UA9apDhCglMJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}