{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b939aded",
      "metadata": {
        "id": "b939aded"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# download data from from Google Drive\n",
        "!gdown 1Rx67QI1zToJT4hwwsh7kryxi3WxuhCvW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file in to a list of tokens\n",
        "from collections import defaultdict\n",
        "noun_tokens=defaultdict(int)\n",
        "verb_tokens=defaultdict(int)\n",
        "tokens=[]\n",
        "tags=[]\n",
        "filename=\"manchester_input_tagged_trf.txt\"\n",
        "with open(filename) as file:\n",
        "        for line in file:\n",
        "            tokens.append(\"{\")\n",
        "            tags.append(\"BOS\")\n",
        "            line_array = line.split()\n",
        "            for element in line_array:\n",
        "                la=re.match(\"([^ ]+)\\_([^ ]+)\",element)\n",
        "                w=la.group(1)\n",
        "                t=la.group(2)\n",
        "                tokens.append(w)\n",
        "                tags.append(t)\n",
        "                if re.match(\"^N\",t):\n",
        "                    noun_tokens[str.lower(w)] += 1\n",
        "                if re.match(\"^V\",t):\n",
        "                    verb_tokens[str.lower(w)] += 1\n",
        "            tokens.append(\"}\")\n",
        "            tags.append(\"EOS\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RpQrzHTRg3-9"
      },
      "id": "RpQrzHTRg3-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_noun_counts=sorted(noun_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "sorted_verb_counts=sorted(verb_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "sorted_noun_tokens=list(zip(*sorted_noun_counts))[0]\n",
        "sorted_verb_tokens=list(zip(*sorted_verb_counts))[0]\n",
        "excluded_nouns=[\"mummy\",\"daddy\",\"john\",\"carl\",\"dominic\"] # proper names\n",
        "excluded_verbs=[\"\"]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if x not in excluded_nouns]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if noun_tokens[x] > verb_tokens[x]]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if x not in excluded_verbs]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if verb_tokens[x] > noun_tokens[x]]\n",
        "seed_set_size = 10\n",
        "noun_seeds=sorted_noun_tokens[0:seed_set_size]\n",
        "verb_seeds=sorted_verb_tokens[0:seed_set_size]\n",
        "type_list=list(noun_seeds)\n",
        "type_list.extend(verb_seeds)\n",
        "tokens.insert(0,\"{\")\n",
        "tokens.insert(len(tokens),\"}\")\n",
        "token_count=len(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "WhnJGJPLifUL"
      },
      "id": "WhnJGJPLifUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bfcf79",
      "metadata": {
        "id": "33bfcf79"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "d = defaultdict(lambda: [0] * (seed_set_size*2))\n",
        "window_size = 2\n",
        "\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
        "            begin = max(i - window_size, 0)\n",
        "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
        "            end  = min(i + window_size, token_count)\n",
        "            # Extract the text from beginning of window to the end\n",
        "            context = tokens[begin: end + 1]\n",
        "            # Remove the target word from its own window\n",
        "            context.remove(tokens[i])\n",
        "            # This is problematic because it will only replace nouns and verbs with tags if they are in canonical form\n",
        "            context=[\"noun\" if noun_seeds.count(w) > 0 else w for w in context]\n",
        "            context=[\"verb\" if verb_seeds.count(w) > 0 else w for w in context]\n",
        "            if type_list.count(word) > 0:\n",
        "                seed_id = type_list.index(word)\n",
        "\n",
        "                p1 = context[1] + \"_X_\" + context[2]\n",
        "                p1 = re.sub(\".+(\\{.+)\",\"\\\\1\",p1)\n",
        "                p1 = re.sub(\"(.+\\}).+\",\"\\\\1\",p1)\n",
        "\n",
        "                p1a = context[1] + \"_X\"\n",
        "                p1a = re.sub(\".+(\\{.+)\",\"\\\\1\",p1a)\n",
        "                p1a = re.sub(\"(.+\\}).+\",\"\\\\1\",p1a)\n",
        "\n",
        "                p2 = \"X_\" + context[2] + \"_\" + context[3]\n",
        "                p2 = re.sub(\".+(\\{.+)\",\"\\\\1\",p2)\n",
        "                p2 = re.sub(\"(.+\\}).+\",\"\\\\1\",p2)\n",
        "\n",
        "                p2a = \"X_\" + context[2]\n",
        "                p2a = re.sub(\".+(\\{.+)\",\"\\\\1\",p2a)\n",
        "                p2a = re.sub(\"(.+\\}).+\",\"\\\\1\",p2a)\n",
        "\n",
        "                p3 = context[0] + \"_\" + context[1] + \"_X\"\n",
        "                p3 = re.sub(\".+(\\{.+)\",\"\\\\1\",p3)\n",
        "                p3 = re.sub(\"(.+\\}).+\",\"\\\\1\",p3)\n",
        "\n",
        "                d[p1][seed_id] += 1\n",
        "                d[p2][seed_id] += 1\n",
        "                d[p3][seed_id] += 1\n",
        "                d[p1a][seed_id] += 1\n",
        "                d[p2a][seed_id] += 1\n",
        "\n",
        "\n",
        "df=pd.DataFrame(d)\n",
        "df.index = type_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jK7THvyvINFm"
      },
      "id": "jK7THvyvINFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorize words using framing context - NEED TO UPDATE TO USE SEPARATE TEST SET AND TO IGNORE SEEDS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DzTgtt-ENoPN"
      },
      "id": "DzTgtt-ENoPN"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "window_size = 2\n",
        "toks_to_ignore = [\"{\",\"}\"]\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "            begin = max(i - window_size, 0)\n",
        "            end  = min(i + window_size, token_count)\n",
        "            context = tokens[begin: end + 1]\n",
        "            context.remove(tokens[i])\n",
        "            context=[\"noun\" if noun_seeds.count(w) > 0 else w for w in context]\n",
        "            context=[\"verb\" if verb_seeds.count(w) > 0 else w for w in context]\n",
        "\n",
        "            if toks_to_ignore.count(word) == 0:\n",
        "                p1 = context[1] + \"_X_\" + context[2]\n",
        "                p1 = re.sub(\".+(\\{.+)\",\"\\\\1\",p1)\n",
        "                p1 = re.sub(\"(.+\\}).+\",\"\\\\1\",p1)\n",
        "                p1a = context[1] + \"_X\"\n",
        "                p1a = re.sub(\".+(\\{.+)\",\"\\\\1\",p1a)\n",
        "                p1a = re.sub(\"(.+\\}).+\",\"\\\\1\",p1a)\n",
        "                # Need to allow frequent trigram here to outrank seed-based pattern\n",
        "                if list(df.columns).count(p1) > 0:\n",
        "                    #print(p1)\n",
        "                    #print(df.get(p1))\n",
        "                    #print(df.get(p1).idxmax())\n",
        "                    if sorted_noun_tokens.count(df.get(p1).idxmax()) > 0:\n",
        "                        category=\"noun\"\n",
        "                    if sorted_verb_tokens.count(df.get(p1).idxmax()) > 0:\n",
        "                        category=\"verb\"\n",
        "                elif list(df.columns).count(p1a) > 0:\n",
        "                    if sorted_noun_tokens.count(df.get(p1a).idxmax()) > 0:\n",
        "                        category=\"noun\"\n",
        "                    if sorted_verb_tokens.count(df.get(p1a).idxmax()) > 0:\n",
        "                        category=\"verb\"\n",
        "                else:\n",
        "                    category = \"other\"\n",
        "                print(word + \" \" + category)\n"
      ],
      "metadata": {
        "id": "UA9apDhCglMJ"
      },
      "id": "UA9apDhCglMJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}