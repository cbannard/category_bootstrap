{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DydvWn5zeo3"
      },
      "outputs": [],
      "source": [
        "!gdown  1bSBMkhXFZLEdZtCAvSexol4d3bXz6wR9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "TjqIyA2Z5Sgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!head -n100000 manchester_input_tagged_trf.txt > test_text"
      ],
      "metadata": {
        "id": "LsFyPb688gN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=\"manchester_input_tagged_trf.txt\"\n",
        "#file=\"test_text\"\n",
        "output=\"manchester_input_tagged_trf_postprocessed.txt\"\n",
        "with open(output, 'w') as fi:\n",
        "    with open(file, 'r') as infile:\n",
        "        lines = infile.readlines()\n",
        "        i = 0\n",
        "        for line in lines:\n",
        "            i = i+1\n",
        "            print(i)\n",
        "            line = line.rstrip()\n",
        "            line = re.sub(\"_PROPN\",\"_NOUN\",line)\n",
        "            line = re.sub(\"gon_([^ ]+) na_([^ ]+)\",\"gonna_VERB\",line)\n",
        "            line = re.sub(\"got_([^ ]+) ta_([^ ]+)\",\"gotta_VERB\",line)\n",
        "            line = re.sub(\"([^ ]+)_AUX\",\"\\\\1_VERB\",line)\n",
        "            line = re.sub(\"([^ ]+)'ll_([^ ]+)\",\"\\\\1_NOUN 'll_VERB\",line)\n",
        "            line = re.sub(\"([a-z])@l_([^ ]+)\",\"\\\\1@l_NOUN\",line)\n",
        "            line = re.sub(\"([^ ]+)_NOUN 's_PART\",\"\\\\1's_NOUN\",line)\n",
        "            line = re.sub(\"([^ ]+)_PRON 's_PART\",\"\\\\1's_PRON\",line)\n",
        "            # We extracted a list of conjoined elements that are tagged as NOUN.\n",
        "            # The following items were judged not to be NOUNs and so are retagged.\n",
        "            line = re.sub(\"night_night_NOUN\",\"night_night_X\",line)\n",
        "            line = re.sub(\"a_lot_of_NOUN\",\"a_lot_of_X\",line)\n",
        "            line = re.sub(\"lots_of_NOUN\",\"lots_of_X\",line)\n",
        "            line = re.sub(\"happy_birthday_NOUN\",\"happy_birthday_X\",line)\n",
        "            line = re.sub(\"as_well_NOUN\",\"as_well_X\",line)\n",
        "            line = re.sub(\"tickle_tickle_NOUN\",\"tickle_tickle_X\",line)\n",
        "            line = re.sub(\"see_saw_marjorie_daw_NOUN\",\"see_saw_marjorie_daw_X\",line)\n",
        "            line = re.sub(\"thank_you_NOUN\",\"thank_you_X\",line)\n",
        "            line = re.sub(\"wakie_wakie_NOUN\",\"wakie_wakie_X\",line)\n",
        "            line = re.sub(\"so_that_NOUN\",\"so_that_X\",line)\n",
        "            line = re.sub(\"as_well_as_NOUN\",\"as_well_as_X\",line)\n",
        "            # We extracted all items that are tagged as NOUN. We checked all items that occured more than 20 times (0.01%).\n",
        "            # Any items that we thought not to be NOUNS were checked.\n",
        "            # The following items were judged not to be NOUNs after checking and so are retagged as X.\n",
        "            line = re.sub(\"(sposta)_NOUN\",\"\\\\1_X\")\n",
        "            line = re.sub(\"(hafta)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(o'clock)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(\\\\'s)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(none)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(hasta)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(pretend)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(-)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(useta)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(yours)_NOUN\",\"\\\\1_X\",line)\n",
        "            line = re.sub(\"(upsidedown)_NOUN\",\"\\\\1_X\",line)\n",
        "\n",
        "            #all WORD'll -> WORD_NOUN 'll_VERB\n",
        "\n",
        "            output += line\n",
        "            output = output + \"\\n\"\n",
        "        fi.write(output)"
      ],
      "metadata": {
        "id": "WzUcbwNG4hX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUX -> VERB\n",
        "PropN -> Noun\n",
        "gon na -> gonna (need to be VERBs)\n",
        "got ta -> gotta (need to be VERBs)\n",
        "all WORD'll -> WORD_NOUN 'll_VERB (currently only does pronoun)\n",
        "\n",
        "all @l elements should be tagged as NOUN\n",
        "\n",
        "possessive s is split - we want to put it back together.\n",
        "\n",
        "Find conjoined elements that are incorrectly tagged as NOUN and retag as X\n",
        "\n",
        "Correct things that are consistently misclassified as NOUN or VERB\n",
        "\n",
        "\n",
        "\n",
        "[Could also check whether item that are sometimes classified as NOUNs and VERBs are also sometimes tagged as anything else. Currently don't do as Brusini et al didn't.]\n"
      ],
      "metadata": {
        "id": "26xn-Ltfz3Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep  \" use_NOUN\" manchester_input_tagged_trf_postprocessed.txt"
      ],
      "metadata": {
        "id": "ON7F5lWigQWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "noun_tokens=defaultdict(int)\n",
        "verb_tokens=defaultdict(int)\n",
        "#tokens_tags=dict()\n",
        "tokens=[]\n",
        "tags=[]\n",
        "filename=\"manchester_input_tagged_trf_postprocessed.txt\"\n",
        "with open(filename) as file:\n",
        "        for line in file:\n",
        "            tokens.append(\"{\")\n",
        "            tags.append(\"BOS\")\n",
        "            line_array = line.split()\n",
        "            for element in line_array:\n",
        "                la=re.match(\"([^ ]+)\\\\_([^ ]+)\",element)\n",
        "                w=la.group(1)\n",
        "                t=la.group(2)\n",
        "                tokens.append(w)\n",
        "                tags.append(t)\n",
        "                if re.match(\"NOUN\",t):\n",
        "                    noun_tokens[str.lower(w)] += 1\n",
        "                if re.match(\"VERB\",t):\n",
        "                    verb_tokens[str.lower(w)] += 1\n",
        "            tokens.append(\"}\")\n",
        "            tags.append(\"EOS\")\n"
      ],
      "metadata": {
        "id": "J7JRN9dQg-Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"as_well_as_NOUN\" manchester_input_tagged_trf_postprocessed.txt"
      ],
      "metadata": {
        "id": "KmMOjXXeiZRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_noun_counts=sorted(noun_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "sorted_verb_counts=sorted(verb_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "sorted_noun_tokens=list(zip(*sorted_noun_counts))[0]\n",
        "sorted_verb_tokens=list(zip(*sorted_verb_counts))[0]\n",
        "excluded_nouns=[\"mummy\",\"daddy\",\"john\",\"carl\",\"dominic\"] # proper names\n",
        "excluded_verbs=[\"\"]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if x not in excluded_nouns]\n",
        "sorted_noun_tokens=[x for x in sorted_noun_tokens if noun_tokens[x] > verb_tokens[x]]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if x not in excluded_verbs]\n",
        "sorted_verb_tokens=[x for x in sorted_verb_tokens if verb_tokens[x] > noun_tokens[x]]\n",
        "seed_set_size = 10\n",
        "noun_seeds=sorted_noun_tokens[0:seed_set_size]\n",
        "verb_seeds=sorted_verb_tokens[0:seed_set_size]\n",
        "type_list=list(noun_seeds)\n",
        "type_list.extend(verb_seeds)\n",
        "tokens.insert(0,\"{\")\n",
        "tokens.insert(len(tokens),\"}\")\n",
        "token_count=len(tokens)"
      ],
      "metadata": {
        "id": "M3w8tCR8hWVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sorted_noun_counts=sorted(noun_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "sorted_noun_tokens=list(zip(*sorted_noun_counts))[0]\n"
      ],
      "metadata": {
        "id": "MeRmxN_wRd_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compounds=pd.DataFrame([[re.sub(\"[\\\\+_]\",\" \",i[0]) for i in sorted_noun_counts if re.search(\"[\\\\+\\\\_]\",i[0])],[i[1] for i in sorted_noun_counts if re.search(\"[\\\\+\\\\_]\",i[0])]]).transpose()"
      ],
      "metadata": {
        "id": "JmoAMYvLRi_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compounds"
      ],
      "metadata": {
        "id": "av0yvfbFTC7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compounds.to_csv(\"compounds.csv\")"
      ],
      "metadata": {
        "id": "Y8jC1EFvSaqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "sorted_noun_counts=pd.DataFrame(data=sorted_noun_counts)\n",
        "sorted_verb_counts=pd.DataFrame(data=sorted_verb_counts)\n",
        "sorted_noun_counts.to_csv(\"nouns_for_checking.csv\")\n",
        "sorted_verb_counts.to_csv(\"verbs_for_checking.csv\")"
      ],
      "metadata": {
        "id": "vJ_b3pWrhcCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "953wumrbpE1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U7gKrt9dkOn9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFN5KpWwjjnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem words:\n",
        "\n",
        "one\n",
        "\n",
        "bit\n",
        "\n",
        "pardon"
      ],
      "metadata": {
        "id": "cyU_M0vBjkIb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbzNGbn4zqXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}